{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:07:54.403984Z",
     "iopub.status.busy": "2025-05-07T04:07:54.403659Z",
     "iopub.status.idle": "2025-05-07T04:07:57.750277Z",
     "shell.execute_reply": "2025-05-07T04:07:57.749301Z",
     "shell.execute_reply.started": "2025-05-07T04:07:54.403956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:07:57.752219Z",
     "iopub.status.busy": "2025-05-07T04:07:57.751888Z",
     "iopub.status.idle": "2025-05-07T04:07:57.757320Z",
     "shell.execute_reply": "2025-05-07T04:07:57.756528Z",
     "shell.execute_reply.started": "2025-05-07T04:07:57.752177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:07:57.759030Z",
     "iopub.status.busy": "2025-05-07T04:07:57.758715Z",
     "iopub.status.idle": "2025-05-07T04:07:57.807774Z",
     "shell.execute_reply": "2025-05-07T04:07:57.807016Z",
     "shell.execute_reply.started": "2025-05-07T04:07:57.759001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_df = pd.read_csv(\"/kaggle/input/hinge-english-to-hinglish-machine-translation/synthetic-dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/hinge-english-to-hinglish-machine-translation/synthetic-dataset/valid.csv\")\n",
    "\n",
    "# Helper: Basic tokenizer\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text.strip().split()\n",
    "\n",
    "# Generate n-grams from token list\n",
    "def get_ngrams(tokens, n):\n",
    "    return [\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def merge_repeated_ngrams(ngrams, n):\n",
    "    if not ngrams:\n",
    "        return \"\"\n",
    "\n",
    "    # Start with the first trigram, split into words\n",
    "    merged = ngrams[0].split()\n",
    "    \n",
    "    for i in range(1, len(ngrams)):\n",
    "        prev = ngrams[i-1].split()\n",
    "        curr = ngrams[i].split()\n",
    "        \n",
    "        # If first two words of current match last two of previous\n",
    "        if len(curr) >= n and prev[-(n-1):] == curr[:(n-1)]:\n",
    "            merged.append(curr[-1])  # Only add the last word\n",
    "        else:\n",
    "            merged.extend(curr)\n",
    "\n",
    "    return \" \".join(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:07:57.808784Z",
     "iopub.status.busy": "2025-05-07T04:07:57.808571Z",
     "iopub.status.idle": "2025-05-07T04:07:57.815326Z",
     "shell.execute_reply": "2025-05-07T04:07:57.814289Z",
     "shell.execute_reply.started": "2025-05-07T04:07:57.808769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build n-gram alignment model\n",
    "class NGramAligner:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.translation_table = defaultdict(Counter)\n",
    "\n",
    "    def train(self, source_sentences, target_sentences):\n",
    "        for src_sent, tgt_sent in zip(source_sentences, target_sentences):\n",
    "            src_tokens = tokenize(src_sent)\n",
    "            tgt_tokens = tokenize(tgt_sent)\n",
    "\n",
    "            src_ngrams = get_ngrams(src_tokens, self.n)\n",
    "            tgt_ngrams = get_ngrams(tgt_tokens, self.n)\n",
    "\n",
    "            for i in range(min(len(src_ngrams), len(tgt_ngrams))):\n",
    "                self.translation_table[src_ngrams[i]][tgt_ngrams[i]] += 1\n",
    " \n",
    "        # Build final dictionary: for each English n-gram, pick the most common Hinglish translation\n",
    "        self.translation_dict = {\n",
    "            eng_ng: tgt_counter.most_common(1)[0][0]\n",
    "            for eng_ng, tgt_counter in self.translation_table.items()\n",
    "        }\n",
    "\n",
    "    def translate(self, sentence):\n",
    "        tokens = tokenize(sentence)\n",
    "        ngrams = get_ngrams(tokens, self.n)\n",
    "        translated = []\n",
    "\n",
    "        for ng in ngrams:\n",
    "            if ng in self.translation_dict:\n",
    "                translated.append(self.translation_dict[ng])\n",
    "            else:\n",
    "                translated.append(ng) \n",
    "\n",
    "        return merge_repeated_ngrams(translated, self.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T04:07:57.817057Z",
     "iopub.status.busy": "2025-05-07T04:07:57.816859Z",
     "iopub.status.idle": "2025-05-07T04:07:57.946500Z",
     "shell.execute_reply": "2025-05-07T04:07:57.945352Z",
     "shell.execute_reply.started": "2025-05-07T04:07:57.817043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Examples:\n",
      "EN: He raised the heavens and set up everything in balance,\n",
      "\n",
      "HI: usne heavens ko everything aur jo kuchh everything kiya aur balance sthapit kiya everything in balance\n",
      "\n",
      "EN: and encourage them to do the same.\n",
      "\n",
      "HI: and encourage them to do jan jati hai do the same\n",
      "\n",
      "EN: And you probably all have it, and if you haven't, you need to.\n",
      "\n",
      "HI: aur shayad aap sabhi men bhi vah hota hai aur agar vah iske lie aapko\n",
      "\n",
      "EN: In their case, it is included in the capitation fee upto a distance of 5 km. between the Clinic of IMP and IP 's residence.\n",
      "\n",
      "HI: unke mamle men ise bima chikitsa vyavsayi ek bar sammilit vyavsayi ke clinic aur ip case ke residence ke bich 5 ki km ki distance tak prti case fee\n",
      "\n",
      "EN: From here the trekkers enter the deep valley and have to cross many small rivulets and huge rocks.\n",
      "\n",
      "HI: cross many small gahri enter the deep karte hain aur unhen marg men kee chhotechhote nale aur barebare\n",
      "\n",
      "Input sentence:\n",
      "he is a treatment ka good a good person\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "aligner = NGramAligner(n=3)  \n",
    "aligner.train(train_df[\"English\"], train_df[\"Hinglish\"])\n",
    "\n",
    "# Test it on some sentences\n",
    "sample_sentences = test_df[\"English\"].sample(5).tolist()\n",
    "\n",
    "print(\"Translation Examples:\")\n",
    "for sent in sample_sentences:\n",
    "    print(f\"EN: {sent}\")\n",
    "    print(f\"HI: {aligner.translate(sent)}\\n\")\n",
    "    \n",
    "print(\"Input sentence:\")\n",
    "print(aligner.translate(\"He is a good person\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2258077,
     "sourceId": 3783258,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
