{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3783258,"sourceType":"datasetVersion","datasetId":2258077}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:03.913898Z","iopub.execute_input":"2025-05-07T08:54:03.914187Z","iopub.status.idle":"2025-05-07T08:54:03.922197Z","shell.execute_reply.started":"2025-05-07T08:54:03.914166Z","shell.execute_reply":"2025-05-07T08:54:03.921329Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/hinge-english-to-hinglish-machine-translation/no_label.csv\n/kaggle/input/hinge-english-to-hinglish-machine-translation/synthetic-dataset/valid.csv\n/kaggle/input/hinge-english-to-hinglish-machine-translation/synthetic-dataset/train.csv\n/kaggle/input/hinge-english-to-hinglish-machine-translation/synthetic-dataset/.ipynb_checkpoints/train-checkpoint.csv\n/kaggle/input/hinge-english-to-hinglish-machine-translation/human-generated-dataset/valid_human_generated.pkl\n/kaggle/input/hinge-english-to-hinglish-machine-translation/human-generated-dataset/train_human_generated.pkl\n","output_type":"stream"}],"execution_count":128},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:03.923452Z","iopub.execute_input":"2025-05-07T08:54:03.923747Z","iopub.status.idle":"2025-05-07T08:54:03.937917Z","shell.execute_reply.started":"2025-05-07T08:54:03.923717Z","shell.execute_reply":"2025-05-07T08:54:03.936892Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"df = pd.read_json(\"hf://datasets/findnitai/english-to-hinglish/hinglish_upload_v1.json\", lines=True)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:03.939131Z","iopub.execute_input":"2025-05-07T08:54:03.939396Z","iopub.status.idle":"2025-05-07T08:54:05.311055Z","shell.execute_reply.started":"2025-05-07T08:54:03.939365Z","shell.execute_reply":"2025-05-07T08:54:05.310068Z"}},"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"                                         translation\n0  {'en': 'What's the name of the movie', 'hi_ng'...\n1  {'en': 'Hi, the rotten tomatoes score is great...\n2  {'en': 'Do you think you will like the movie',...\n3  {'en': 'What kind of movie is it', 'hi_ng': 'y...\n4  {'en': 'when was the movie made?', 'hi_ng': 'f...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{'en': 'What's the name of the movie', 'hi_ng'...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{'en': 'Hi, the rotten tomatoes score is great...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>{'en': 'Do you think you will like the movie',...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>{'en': 'What kind of movie is it', 'hi_ng': 'y...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>{'en': 'when was the movie made?', 'hi_ng': 'f...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":130},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Step 1: Keep only rows where 'translation' is a dictionary\ndf = df[df['translation'].apply(lambda x: isinstance(x, dict))].copy()\n\n# Step 2: Normalize the dictionary to separate columns\nnormalized_df = pd.json_normalize(df['translation'])\n\n# Step 3: Rename columns\nnormalized_df = normalized_df.rename(columns={'en': 'English', 'hi_ng': 'Hinglish'})\n\n# Step 4: Drop the original 'translation' column and merge\ndf = df.drop(columns=['translation']).join(normalized_df)\n\n# Step 5: Drop any rows with missing English or Hinglish translations\ndf = df.dropna(subset=['English', 'Hinglish'])\n\n# Step 6: Split into train and validation sets\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\n# Optional: Display results\nprint(\"Train set:\\n\", train_df.head())\nprint(\"Validation set:\\n\", val_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:05.312152Z","iopub.execute_input":"2025-05-07T08:54:05.312440Z","iopub.status.idle":"2025-05-07T08:54:06.170005Z","shell.execute_reply.started":"2025-05-07T08:54:05.312417Z","shell.execute_reply":"2025-05-07T08:54:06.168929Z"}},"outputs":[{"name":"stdout","text":"Train set:\n                                              English  \\\n0                             has my timer started ?   \n1                      Set a weekend alarm for 10 am   \n2  Show Christmas lighting events near me this month   \n3  Should I avoid any highways on the way to New ...   \n4                         how sunny will it be today   \n\n                                            Hinglish  source  \n0                            mera timer shuru hoga ?       0  \n1           10 am ke liye weekend alarm ko set kardo       1  \n2  is mahine mere aas pas ke Christmas lighting e...       0  \n3  Kya muje New York jaate samay raaste me koi hi...       0  \n4                       Aaj kitna sunny hone wala he       0  \nValidation set:\n                                              English  \\\n0  tell my boyfriend to meet for drinks at Outbac...   \n1  i need to get rid of all of my weekend reminde...   \n2  Please show me alternative routes to Northeast...   \n3  how long will it take me to get to boston at 6...   \n4                         What are my new messages ?   \n\n                                            Hinglish  source  \n0  mere pati ko aaj raat Outback me drinks par mi...       0  \n1  mujhe mere sabhi weekend reminders se agle mah...       0  \n2  Please mujhe freeway ke bina Northeast Pediatr...       0  \n3  60 miles per hour par mujhe boston pahuchne me...       0  \n4                       Mere naye messages kya hai ?       0  \n","output_type":"stream"}],"execution_count":131},{"cell_type":"code","source":"print(len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:06.171024Z","iopub.execute_input":"2025-05-07T08:54:06.171312Z","iopub.status.idle":"2025-05-07T08:54:06.175961Z","shell.execute_reply.started":"2025-05-07T08:54:06.171277Z","shell.execute_reply":"2025-05-07T08:54:06.175025Z"}},"outputs":[{"name":"stdout","text":"189102\n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    DataCollatorForSeq2Seq\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:06.177918Z","iopub.execute_input":"2025-05-07T08:54:06.178181Z","iopub.status.idle":"2025-05-07T08:54:06.190913Z","shell.execute_reply.started":"2025-05-07T08:54:06.178150Z","shell.execute_reply":"2025-05-07T08:54:06.190337Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"import torch\n\n# Force use of only one CUDA device (cuda:0)\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)  # Ensures only CUDA device 0 is used\n    device = torch.device(\"cuda:0\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:06.192441Z","iopub.execute_input":"2025-05-07T08:54:06.192740Z","iopub.status.idle":"2025-05-07T08:54:06.206297Z","shell.execute_reply.started":"2025-05-07T08:54:06.192712Z","shell.execute_reply":"2025-05-07T08:54:06.205423Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:0\n","output_type":"stream"}],"execution_count":134},{"cell_type":"code","source":"# Load CSV containing 'English' and 'Hinglish' columns\ntrain_dataset = Dataset.from_pandas(train_df[['English', 'Hinglish']])\ntest_dataset = Dataset.from_pandas(val_df[['English', 'Hinglish']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:06.207005Z","iopub.execute_input":"2025-05-07T08:54:06.207191Z","iopub.status.idle":"2025-05-07T08:54:06.418008Z","shell.execute_reply.started":"2025-05-07T08:54:06.207174Z","shell.execute_reply":"2025-05-07T08:54:06.417331Z"}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"print(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:06.418940Z","iopub.execute_input":"2025-05-07T08:54:06.419270Z","iopub.status.idle":"2025-05-07T08:54:06.424053Z","shell.execute_reply.started":"2025-05-07T08:54:06.419216Z","shell.execute_reply":"2025-05-07T08:54:06.423156Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['English', 'Hinglish'],\n    num_rows: 151281\n})\n","output_type":"stream"}],"execution_count":136},{"cell_type":"code","source":"# Load tokenizer and model\nmodel_name = \"ai4bharat/INDICBART\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:06.424898Z","iopub.execute_input":"2025-05-07T08:54:06.425127Z","iopub.status.idle":"2025-05-07T08:54:08.131104Z","shell.execute_reply.started":"2025-05-07T08:54:06.425107Z","shell.execute_reply":"2025-05-07T08:54:08.130452Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/config.json\nModel config MBartConfig {\n  \"_name_or_path\": \"ai4bharat/INDICBART\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"architectures\": [\n    \"MBartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 64000,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 64001,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"mbart\",\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"scale_embedding\": false,\n  \"tokenizer_class\": \"AlbertTokenizer\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64014\n}\n\nloading file spiece.model from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/spiece.model\nloading file tokenizer.json from cache at None\nloading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/added_tokens.json\nloading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/special_tokens_map.json\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/config.json\nModel config MBartConfig {\n  \"_name_or_path\": \"ai4bharat/INDICBART\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"architectures\": [\n    \"MBartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 64000,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 64001,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"mbart\",\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"scale_embedding\": false,\n  \"tokenizer_class\": \"AlbertTokenizer\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64014\n}\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/config.json\nModel config MBartConfig {\n  \"_name_or_path\": \"ai4bharat/INDICBART\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"architectures\": [\n    \"MBartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 64000,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 64001,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"mbart\",\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"scale_embedding\": false,\n  \"tokenizer_class\": \"AlbertTokenizer\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64014\n}\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/config.json\nModel config MBartConfig {\n  \"_name_or_path\": \"ai4bharat/INDICBART\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"architectures\": [\n    \"MBartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 64000,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 64001,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"mbart\",\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"scale_embedding\": false,\n  \"tokenizer_class\": \"AlbertTokenizer\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64014\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--ai4bharat--INDICBART/snapshots/78466a0c0e29f9229f7005623ecd6bc4243c0ae0/pytorch_model.bin\nGenerate config GenerationConfig {\n  \"bos_token_id\": 64000,\n  \"eos_token_id\": 64001,\n  \"forced_eos_token_id\": 2,\n  \"pad_token_id\": 0\n}\n\nAll model checkpoint weights were used when initializing MBartForConditionalGeneration.\n\nAll the weights of MBartForConditionalGeneration were initialized from the model checkpoint at ai4bharat/INDICBART.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\nGeneration config file not found, using a generation config created from the model config.\n","output_type":"stream"}],"execution_count":137},{"cell_type":"code","source":"def preprocess(example):\n    inputs = tokenizer(\n        example[\"English\"], padding=\"max_length\", truncation=True, max_length=128\n    )\n    targets = tokenizer(\n        example[\"Hinglish\"], padding=\"max_length\", truncation=True, max_length=128\n    )\n    inputs[\"labels\"] = targets[\"input_ids\"]\n\n    # Only return relevant fields\n    return {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"],\n        \"labels\": inputs[\"labels\"]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:08.132012Z","iopub.execute_input":"2025-05-07T08:54:08.132217Z","iopub.status.idle":"2025-05-07T08:54:08.136741Z","shell.execute_reply.started":"2025-05-07T08:54:08.132200Z","shell.execute_reply":"2025-05-07T08:54:08.135801Z"}},"outputs":[],"execution_count":138},{"cell_type":"code","source":"# Apply preprocessing\ntokenized_train_dataset = train_dataset.map(preprocess, batched=True)\ntokenized_test_dataset = test_dataset.map(preprocess, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:08.137630Z","iopub.execute_input":"2025-05-07T08:54:08.137926Z","iopub.status.idle":"2025-05-07T08:54:39.866773Z","shell.execute_reply.started":"2025-05-07T08:54:08.137897Z","shell.execute_reply":"2025-05-07T08:54:39.866058Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/151281 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23766ec0758246609ee02849d33b955d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/37821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dd6203f1d1b493f92f10cc668a966bf"}},"metadata":{}}],"execution_count":139},{"cell_type":"code","source":"# Data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:39.867771Z","iopub.execute_input":"2025-05-07T08:54:39.868020Z","iopub.status.idle":"2025-05-07T08:54:39.965846Z","shell.execute_reply.started":"2025-05-07T08:54:39.867986Z","shell.execute_reply":"2025-05-07T08:54:39.965059Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"from transformers import logging\n\n# Suppress extra warnings (optional)\nlogging.set_verbosity_info()\n\n# Show tqdm progress bar in Kaggle\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./indicbart_hinglish\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=1e-3,\n    num_train_epochs=1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    predict_with_generate=True,\n    save_total_limit=2,\n    logging_steps=10,       \n    report_to=\"none\",       \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:39.968965Z","iopub.execute_input":"2025-05-07T08:54:39.969234Z","iopub.status.idle":"2025-05-07T08:54:40.004871Z","shell.execute_reply.started":"2025-05-07T08:54:39.969212Z","shell.execute_reply":"2025-05-07T08:54:40.004239Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}],"execution_count":141},{"cell_type":"code","source":"# Trainer (updated)\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_test_dataset,\n    data_collator=data_collator,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:40.006219Z","iopub.execute_input":"2025-05-07T08:54:40.006589Z","iopub.status.idle":"2025-05-07T08:54:40.040058Z","shell.execute_reply.started":"2025-05-07T08:54:40.006558Z","shell.execute_reply":"2025-05-07T08:54:40.039506Z"}},"outputs":[],"execution_count":142},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T08:54:40.041349Z","iopub.execute_input":"2025-05-07T08:54:40.041645Z","iopub.status.idle":"2025-05-07T10:27:57.644749Z","shell.execute_reply.started":"2025-05-07T08:54:40.041616Z","shell.execute_reply":"2025-05-07T10:27:57.643703Z"}},"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Hinglish, English. If Hinglish, English are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 151,281\n  Num Epochs = 1\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 9,456\n  Number of trainable parameters = 244,017,152\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9456' max='9456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9456/9456 1:33:16, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.090400</td>\n      <td>0.085314</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./indicbart_hinglish/checkpoint-9456\nConfiguration saved in ./indicbart_hinglish/checkpoint-9456/config.json\nConfiguration saved in ./indicbart_hinglish/checkpoint-9456/generation_config.json\nModel weights saved in ./indicbart_hinglish/checkpoint-9456/model.safetensors\nThe following columns in the evaluation set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: Hinglish, English. If Hinglish, English are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 37821\n  Batch size = 16\nSaving model checkpoint to ./indicbart_hinglish/checkpoint-9456\nConfiguration saved in ./indicbart_hinglish/checkpoint-9456/config.json\nConfiguration saved in ./indicbart_hinglish/checkpoint-9456/generation_config.json\nModel weights saved in ./indicbart_hinglish/checkpoint-9456/model.safetensors\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":143,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9456, training_loss=0.14130312286304741, metrics={'train_runtime': 5597.0777, 'train_samples_per_second': 27.029, 'train_steps_per_second': 1.689, 'total_flos': 2.0490823290322944e+16, 'train_loss': 0.14130312286304741, 'epoch': 1.0})"},"metadata":{}}],"execution_count":143},{"cell_type":"code","source":"# Save model and tokenizer\nmodel.save_pretrained(\"./indicbart_hinglish_final\")\ntokenizer.save_pretrained(\"./indicbart_hinglish_final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:27:57.645770Z","iopub.execute_input":"2025-05-07T10:27:57.646131Z","iopub.status.idle":"2025-05-07T10:28:00.973669Z","shell.execute_reply.started":"2025-05-07T10:27:57.646085Z","shell.execute_reply":"2025-05-07T10:28:00.972796Z"}},"outputs":[{"name":"stderr","text":"Configuration saved in ./indicbart_hinglish_final/config.json\nConfiguration saved in ./indicbart_hinglish_final/generation_config.json\nModel weights saved in ./indicbart_hinglish_final/model.safetensors\ntokenizer config file saved in ./indicbart_hinglish_final/tokenizer_config.json\nSpecial tokens file saved in ./indicbart_hinglish_final/special_tokens_map.json\n","output_type":"stream"},{"execution_count":144,"output_type":"execute_result","data":{"text/plain":"('./indicbart_hinglish_final/tokenizer_config.json',\n './indicbart_hinglish_final/special_tokens_map.json',\n './indicbart_hinglish_final/spiece.model',\n './indicbart_hinglish_final/added_tokens.json',\n './indicbart_hinglish_final/tokenizer.json')"},"metadata":{}}],"execution_count":144},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"./indicbart_hinglish_final\")\ntokenizer = AutoTokenizer.from_pretrained(\"./indicbart_hinglish_final\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:28:00.974615Z","iopub.execute_input":"2025-05-07T10:28:00.974846Z","iopub.status.idle":"2025-05-07T10:28:05.898843Z","shell.execute_reply.started":"2025-05-07T10:28:00.974815Z","shell.execute_reply":"2025-05-07T10:28:05.897974Z"}},"outputs":[{"name":"stderr","text":"loading configuration file ./indicbart_hinglish_final/config.json\nModel config MBartConfig {\n  \"_name_or_path\": \"./indicbart_hinglish_final\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"architectures\": [\n    \"MBartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 64000,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"dropout\": 0.1,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 64001,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"mbart\",\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 0,\n  \"scale_embedding\": false,\n  \"tokenizer_class\": \"AlbertTokenizer\",\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 64014\n}\n\nloading weights file ./indicbart_hinglish_final/model.safetensors\nGenerate config GenerationConfig {\n  \"bos_token_id\": 64000,\n  \"eos_token_id\": 64001,\n  \"forced_eos_token_id\": 2,\n  \"pad_token_id\": 0\n}\n\nAll model checkpoint weights were used when initializing MBartForConditionalGeneration.\n\nAll the weights of MBartForConditionalGeneration were initialized from the model checkpoint at ./indicbart_hinglish_final.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\nloading configuration file ./indicbart_hinglish_final/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 64000,\n  \"eos_token_id\": 64001,\n  \"forced_eos_token_id\": 2,\n  \"pad_token_id\": 0\n}\n\nloading file spiece.model\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}],"execution_count":145},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:28:05.899545Z","iopub.execute_input":"2025-05-07T10:28:05.899791Z","iopub.status.idle":"2025-05-07T10:28:07.166025Z","shell.execute_reply.started":"2025-05-07T10:28:05.899770Z","shell.execute_reply":"2025-05-07T10:28:07.165334Z"}},"outputs":[],"execution_count":146},{"cell_type":"code","source":"def translate(text):\n    model.eval()\n\n    # Move tokenizer inputs to the same device\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n\n    # Generate translation\n    with torch.no_grad():\n        output_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=5)\n\n    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:28:07.166832Z","iopub.execute_input":"2025-05-07T10:28:07.167050Z","iopub.status.idle":"2025-05-07T10:28:09.138817Z","shell.execute_reply.started":"2025-05-07T10:28:07.167031Z","shell.execute_reply":"2025-05-07T10:28:09.137736Z"}},"outputs":[],"execution_count":147},{"cell_type":"code","source":"print(\"has my timer started ?\")\nprint(translate(\"has my timer started ?\"))\nprint(\"\\nset an alarm for me\")\nprint(translate(\"set an alarm for me\"))\nprint(\"\\nDid I get new messages ?\")\nprint(translate(\"Did I get new messages ?\"))\nprint(\"\\nWhat is the time right now ?\")\nprint(translate(\"What is the time right now ?\"))\nprint(\"\\nIt will be sunny today\")\nprint(translate(\"It will be sunny today\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:28:09.139752Z","iopub.execute_input":"2025-05-07T10:28:09.140025Z","iopub.status.idle":"2025-05-07T10:28:14.844182Z","shell.execute_reply.started":"2025-05-07T10:28:09.139999Z","shell.execute_reply":"2025-05-07T10:28:14.843334Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"has my timer started ?\nkya mera timer shuru hoga ?\n\nset an alarm for me\nmere liye ek alarm set karen\n\nDid I get new messages ?\nkya maine naye messages milgaye hai ?\n\nWhat is the time right now ?\nabhi ka time kya hai ?\n\nIt will be sunny today\nAaj dhoop hogi\n","output_type":"stream"}],"execution_count":148},{"cell_type":"code","source":"print(\"My smartwatch just died in the middle of a workout.\")\nprint(translate(\"My smartwatch just died in the middle of a workout.\"))\n\nprint(\"\\nThe Wi-Fi router is acting up again.\")\nprint(translate(\"The Wi-Fi router is acting up again.\"))\n\nprint(\"\\nI need to clear my browser history.\")\nprint(translate(\"I need to clear my browser history.\"))\n\nprint(\"\\nI forgot to cancel my subscription to that streaming service.\")\nprint(translate(\"I forgot to cancel my subscription to that streaming service.\"))\n\nprint(\"\\nThe game crashed right before I reached the final boss.\")\nprint(translate(\"The game crashed right before I reached the final boss.\"))\n\nprint(\"\\nMy phone froze when I was about to check an important message.\")\nprint(translate(\"My phone froze when I was about to check an important message.\"))\n\nprint(\"\\nI need to fix the bug in my code before the deadline.\")\nprint(translate(\"I need to fix the bug in my code before the deadline.\"))\n\nprint(\"\\nI'm trying to get my hands on the new gaming console.\")\nprint(translate(\"I'm trying to get my hands on the new gaming console.\"))\n\nprint(\"\\nI ordered food online, but they gave me the wrong item.\")\nprint(translate(\"I ordered food online, but they gave me the wrong item.\"))\n\nprint(\"\\nI need a caffeine boost to survive this meeting.\")\nprint(translate(\"I need a caffeine boost to survive this meeting.\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:30:30.256620Z","iopub.execute_input":"2025-05-07T10:30:30.256951Z","iopub.status.idle":"2025-05-07T10:30:39.454708Z","shell.execute_reply.started":"2025-05-07T10:30:30.256925Z","shell.execute_reply":"2025-05-07T10:30:39.453790Z"}},"outputs":[{"name":"stdout","text":"My smartwatch just died in the middle of a workout.\nmeri smartwatch just died in the middle of a workout.\n\nThe Wi-Fi router is acting up again.\nWi-Fi router fir se chal raha hai.\n\nI need to clear my browser history.\nmujhe mere browser history clear karna hai\n\nI forgot to cancel my subscription to that streaming service.\nI forgot to cancel my subscription to that streaming service.\n\nThe game crashed right before I reached the final boss.\nThe game crashed right before I reached the final boss.\n\nMy phone froze when I was about to check an important message.\nmera phone froze kab tha jab mujhe important message check karne ke liye\n\nI need to fix the bug in my code before the deadline.\nmujhe deadline se pehle mere code me bug fix karna hai\n\nI'm trying to get my hands on the new gaming console.\nmai new gaming console par apne hands laana chahta hoon\n\nI ordered food online, but they gave me the wrong item.\nI ordered food online, but they gave me the wrong item.\n\nI need a caffeine boost to survive this meeting.\nmujhe is meeting me caffeine boost chahiye.\n","output_type":"stream"}],"execution_count":151},{"cell_type":"code","source":"!pip install evaluate sacrebleu nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:28:14.844930Z","iopub.execute_input":"2025-05-07T10:28:14.845148Z","iopub.status.idle":"2025-05-07T10:28:18.507281Z","shell.execute_reply.started":"2025-05-07T10:28:14.845130Z","shell.execute_reply":"2025-05-07T10:28:18.506302Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\nRequirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.5.1)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (3.1.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":149},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:39:23.996550Z","iopub.execute_input":"2025-05-07T10:39:23.996882Z","iopub.status.idle":"2025-05-07T10:39:24.005205Z","shell.execute_reply.started":"2025-05-07T10:39:23.996859Z","shell.execute_reply":"2025-05-07T10:39:24.004334Z"}},"outputs":[{"execution_count":155,"output_type":"execute_result","data":{"text/plain":"MBartForConditionalGeneration(\n  (model): MBartModel(\n    (shared): MBartScaledWordEmbedding(64014, 1024, padding_idx=0)\n    (encoder): MBartEncoder(\n      (embed_tokens): MBartScaledWordEmbedding(64014, 1024, padding_idx=0)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-5): 6 x MBartEncoderLayer(\n          (self_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): MBartDecoder(\n      (embed_tokens): MBartScaledWordEmbedding(64014, 1024, padding_idx=0)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-5): 6 x MBartDecoderLayer(\n          (self_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartSdpaAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=64014, bias=False)\n)"},"metadata":{}}],"execution_count":155},{"cell_type":"code","source":"# import pandas as pd\n# import torch\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# # Load CSV\n# df = pd.read_csv(\"/kaggle/input/hinge-english-to-hinglish-machine-translation/synthetic-dataset/valid.csv\")\n# df_sample = df.sample(n=50, random_state=42)  # random_state ensures reproducibility\n\n# # Choose source column (e.g., \"English\")\n# sentences = df[\"English\"].astype(str).tolist()\n\n# # Tokenize\n# inputs = tokenizer(\n#     sentences,\n#     return_tensors=\"pt\",\n#     padding=True,\n#     truncation=True,\n#     max_length=100\n# )\n# inputs = {k: v.to(device) for k, v in inputs.items()}\n\n# device = torch.device(\"cpu\")\n# model = model.to(device)\n# for k in inputs:\n#     inputs[k] = inputs[k].to(device)\n\n# inputs.pop(\"token_type_ids\", None)\n\n# with torch.no_grad():\n#     outputs = model.generate(**inputs, max_length=128, num_beams=5)\n\n# # Remove token_type_ids if present\n# if 'token_type_ids' in inputs:\n#     inputs.pop('token_type_ids')\n\n# # Generate translations\n# with torch.no_grad():\n#     outputs = model.generate(**inputs, max_length=128, num_beams=5)\n\n# # Decode predictions\n# preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n# # Optionally, add to DataFrame and save\n# df[\"Translated\"] = preds\n# df.to_csv(\"translated_output.csv\", index=False)\n\n# print(\"Translations saved to translated_output.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:39:37.533139Z","iopub.execute_input":"2025-05-07T10:39:37.533476Z","execution_failed":"2025-05-07T10:47:04.678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install rouge_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:28:20.315428Z","iopub.status.idle":"2025-05-07T10:28:20.315827Z","shell.execute_reply":"2025-05-07T10:28:20.315659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import evaluate\n\n# # Get references from your CSV\n# references = df[\"Hinglish\"].astype(str).tolist()\n\n# # Make sure both preds and references are aligned\n# assert len(preds) == len(references), \"Mismatch between predictions and references.\"\n\n# # BLEU\n# bleu = evaluate.load(\"bleu\")\n# print(\"BLEU:\", bleu.compute(predictions=preds, references=[[ref] for ref in references]))\n\n# # SacreBLEU\n# sacrebleu = evaluate.load(\"sacrebleu\")\n# print(\"SacreBLEU:\", sacrebleu.compute(predictions=preds, references=[[ref] for ref in references]))\n\n# # chrF\n# chrf = evaluate.load(\"chrf\")\n# print(\"chrF:\", chrf.compute(predictions=preds, references=[[ref] for ref in references]))\n\n# # ROUGE\n# rouge = evaluate.load(\"rouge\")\n# print(\"ROUGE:\", rouge.compute(predictions=preds, references=references))\n\n# # Exact Match Accuracy\n# exact_match = [int(p.strip() == r.strip()) for p, r in zip(preds, references)]\n# print(\"Exact Match Accuracy:\", sum(exact_match) / len(exact_match))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T10:28:20.316466Z","iopub.status.idle":"2025-05-07T10:28:20.316817Z","shell.execute_reply":"2025-05-07T10:28:20.316667Z"}},"outputs":[],"execution_count":null}]}